owow-talents-ai-mp
===============

A Python-based ETL pipeline and Fast API service with multi-db setup
---------------

This repo contains code for the take-home assignment for the role of GCP data engineer at OWOW talents.

## ğŸ“‚ Project Structure
â”œâ”€â”€ src/   
â”‚  â”œâ”€â”€ pipeline/&ensp;&ensp;# ETL / orchestration scripts              
â”‚  â”œâ”€â”€ api/&ensp;&ensp;&ensp;# FastAPI app   
â”‚  â”œâ”€â”€ utils/                  
â”‚  â”œâ”€â”€ data/  # contains the milvus lite db and sample file used in pipeline  
â”‚  â”œâ”€â”€ secrets/&ensp;&ensp;&ensp;# store your SA key json for GCP project  
â”‚  â”œâ”€â”€ docker-compose.yml&ensp;&ensp;&ensp; Docker compose file             
â”‚  â”œâ”€â”€ README.md   
â”‚  â”œâ”€â”€ architecture_diagram.png   
â”‚  â”œâ”€â”€ architecture.md   
â””  â””â”€â”€ scaling_plan.md  

## â„¹ï¸ Database details

Code repo contains the following DB setup
1. Raw chat data from users (user_id, message, timestamp) ----> Mongodb
2. 1024-dim vector embeddings ----> Milvus Lite
3. Relationship mappings linking users, campaigns, etc. ----> Neo4j
4. Analytics, aggregated interaction data ----> BigQuery

## â„¹ï¸ ETL-pipeline
1. This code sets up the databases.
2. Loads the data from csv stored on GCS.
3. Creates a relationship in the Neo4j DB
4. Loads vector embeddings into Milvus Lite
5. Uploads CSV data into BigQuery table for analytics

## â„¹ï¸ Fast API service
1. Runs the queries against the data in db's created by the ETL pipeline
2. Has a Fast API service that implements the GET HTTP method to retrieve the query results 
3. The following are the queries 
  â— Retrieve top 5 most similar users (via Milvus vector search).
  â— Fetch campaigns connected to those users (via Neo4j).
  â— Return results ranked by engagement frequency (from analytics DB).

## ğŸ“¦ Deployment Guide
### Prerequisites before execution on the cloud
#### â— Make sure you have Docker Desktop installed.
#### â— Fill the config as per your GCP project details
#### â— Upload your SA JSON in secrets/ dir
#### â— Make sure the service account you are using has the following permissions: 1. roles/bigquery.jobUser 2. roles/storage.objectUser
#### â— Run "docker compose up" at the root directory.
Notes: 
1. Once Docker Compose has run successfully, you will see the containers in Docker Desktop.
2. No need to run docker compose up every time you make changes to the code. Just restart the relevant service container.

